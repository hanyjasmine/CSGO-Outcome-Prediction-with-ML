# -*- coding: utf-8 -*-
"""CSgogogogo

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDTKOJn25gEKHd0f0J8_zkTzNJOpdON5

First, we mount to our Google Drive
"""

# Mount to the drive
from google.colab import drive
drive.mount('/content/gdrive')

# Import packages 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(rc={"figure.dpi":300, 'savefig.dpi':300})
sns.set_context('notebook')
sns.set_style("ticks")
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from statistics import mean

"""Reading in the dataset"""

# Our dataset 
tb_player = pd.read_csv('/content/gdrive/MyDrive/BA476_SP22/Dataset /tb_players.csv')
tb_lobby_stats_player = pd.read_csv('/content/gdrive/MyDrive/BA476_SP22/Dataset /tb_lobby_stats_player.csv')
tb_medalha = pd.read_csv('/content/gdrive/MyDrive/BA476_SP22/Dataset /tb_medalha.csv')
tb_players_medalha = pd.read_csv('/content/gdrive/MyDrive/BA476_SP22/Dataset /tb_players_medalha.csv')

"""### Overview of our dataset

"""

tb_player.head()

tb_lobby_stats_player.head()

tb_medalha.head()

tb_players_medalha.head()

"""### Basic summary of the dataset"""

#dataset basic info 
info = pd.DataFrame({'Dataset' : ['tb_player','tb_lobby_stats_player','tb_medalha','tb_players_medalha'], 
                     'shape':[tb_player.shape, tb_lobby_stats_player.shape,tb_medalha.shape,tb_players_medalha.shape]})
info

#player's level histogram 
plt.hist(tb_lobby_stats_player['vlLevel'])

tb_lobby_stats_player['vlLevel'].unique()

BirthdayNaRate = tb_player['dtBirth'].isna().sum() / tb_player['dtBirth'].size
plyerBirth = tb_player['dtBirth'].dropna().array

age = []
for index in range(plyerBirth.size):
  year = 2022 - int(str(plyerBirth[index])[0:4])
  age.append(year)

plt.hist(age)

#playercountry = tb_player['descCountry'].array
Sumplayercountry = pd.DataFrame(tb_player[['descCountry','flTwitter']]).groupby('descCountry').count()

Sumplayercountry = pd.DataFrame(Sumplayercountry).reset_index()
Sumplayercountry.columns = ['Country','count']
Sumplayercountry

"""Features selection"""

fig = plt.figure(figsize=(25,25))
corrMatrix = tb_lobby_stats_player.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

"""###  Feature Engineering/Data Prep"""

fullcol = ['idLobbyGame', 'idPlayer', 'idRoom', 'qtKill', 'qtAssist', 'qtDeath',
       'qtHs', 'qtBombeDefuse', 'qtBombePlant', 'qtTk', 'qtTkAssist',
       'qt1Kill', 'qt2Kill', 'qt3Kill', 'qt4Kill', 'qt5Kill', 'qtPlusKill',
       'qtFirstKill', 'vlDamage', 'qtHits', 'qtShots', 'qtLastAlive',
       'qtClutchWon', 'qtRoundsPlayed', 'descMapName', 'vlLevel', 'qtSurvived',
       'qtTrade', 'qtFlashAssist', 'qtHitHeadshot', 'qtHitChest',
       'qtHitStomach', 'qtHitLeftAtm', 'qtHitRightArm', 'qtHitLeftLeg',
       'qtHitRightLeg']
fullnum_col = ['qtKill', 'qtAssist', 'qtDeath',
       'qtHs', 'qtBombeDefuse', 'qtBombePlant', 'qtTk', 'qtTkAssist',
       'qt1Kill', 'qt2Kill', 'qt3Kill', 'qt4Kill', 'qt5Kill', 'qtPlusKill',
        'vlDamage', 'qtHits', 'qtShots', 'qtRoundsPlayed','vlLevel', 'qtSurvived',
       'qtTrade', 'qtFlashAssist', 'qtHitHeadshot', 'qtHitChest',
       'qtHitStomach', 'qtHitLeftAtm', 'qtHitRightArm', 'qtHitLeftLeg','qtHitRightLeg']
workcol = ['qtKill', 'qtAssist', 'qtHs', 'qtBombePlant','qt1Kill','qt2Kill','qt3Kill','qtFirstKill',
           'vlDamage','qtHits','qtShots','qtClutchWon','qtSurvived','qtHitHeadshot','qtHitChest','qtHitStomach']
num_col = ['qtKill', 'qtAssist', 'qtHs', 'qtBombePlant','qt1Kill','qt2Kill','qt3Kill','vlDamage','qtHits','qtShots','qtSurvived','qtHitHeadshot','qtHitChest','qtHitStomach']

"""* There are 184,152 rows in our data set, and 705 of them contain Nan values.
* Since we have a bulk dataset, we decided to eliminate the Nan values and save the data set named 'sample'. 
* Moreover, according to our heatmap, we can conclude that most of the variables have weak relations with our estimator 'flWinner'. 
* Therefore, we selected the variables which have index more than 0.1 from the heatmap matrix. 
* For next step, we excluded the outlier in the numerical columns (27,211 rows of dataset contain outliers)
* Dummy varibales: qtFirstKill; qtLastAlive; qtClutchWon;descMapName
"""

#data cleaning
#drop NaN value in the dataset, since we have a huge amount of dataset, since we already have 
sample = tb_lobby_stats_player.dropna(axis = 0)

# only qtFirstKill qtClutchWon are categorical column
print(sample.shape[0])
q75, q25 = np.percentile(sample['vlDamage'],[75,25])
iqr = q75-q25
highb = q75 + 1.5* iqr
lowb = q25 - 1.5 * iqr
sample = sample[(sample['vlDamage']>lowb) & (sample['vlDamage']<highb)]

print(sample.shape[0])

# Adding some interesting features
sample['percSurvived'] = sample['qtSurvived']/sample['qtRoundsPlayed']
sample['percPlantedBomb'] = sample['qtBombePlant']/sample['qtRoundsPlayed']
sample['K/D'] = sample['qtKill']/sample['qtDeath']

# Using Seaborn's regplot function to explore some relationships
sns.regplot(x='percSurvived', y='flWinner', data=sample, logistic=True, ci=None).set(title='Logistic Regression of Percentage Survived vs Match Won')

# Exploring Map Popularity
sns.catplot(x="descMapName", y='qtRoundsPlayed', kind="box", data=sample).set(title = 'Distribution of Rounds Played Across the Different Maps')

# Historgram depicting the percent who survived in relation to winning the match
sns.displot(sample, x = 'percSurvived', hue = 'flWinner', element = 'step', bins = 25).set(title='Histogram Depicting Percentage Survived')
plt.axvline(x=sample.percSurvived.median(),
            color='red')

# Density representation of kill count with winning the match
sns.displot(sample, x = 'qtKill', hue = 'flWinner', kind = 'kde', fill = True).set(title='Density Estimate of Kill Count in Relation to Match Won')
plt.axvline(x=sample.qtKill.median(),
            color='red')

# Similarly, below is a density representation of damage dealt in relation with winning the match
sns.displot(sample, x = 'vlDamage', hue = 'flWinner', kind = 'kde', fill = True).set(title='Density Estimate of Damage Dealt in Relation to Match Won')
plt.axvline(x=sample.vlDamage.median(),
            color='red')

# Death count shows that those that lost the match had more deaths
sns.displot(sample, x = 'qtDeath', hue = 'flWinner', kind = 'kde', fill = True).set(title='Density Estimate of Death Count in Relation to Match Won')
plt.axvline(x=sample.qtKill.median(),
            color='red')

"""prepare dataset"""

from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler 

X = sample[fullcol]
X['descMapName'],_ = pd.factorize(X['descMapName'])
y = sample['flWinner']


#transfer string to numerical 
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)  

sub_X_train = X_train[workcol]
sub_X_test = X_test[workcol]

num_X_train = X_train[fullnum_col]
num_X_test = X_test[fullnum_col]


scaler = StandardScaler() 
X1_std_train = scaler.fit_transform(X_train)
X1_std_test = scaler.fit_transform(X_test)

X1_std_train[:,:].std(axis=0), X1_std_train[:,:].mean(axis=0)

"""## modeling

Model baseline - using the  [Zero Rule Algorithm](https:/machinelearningmasterycomimplement-baseline-machine-learning-algorithms-scratch-python/), there are 50.4% of the game end with defeat, which mean that 50.4% in column 'flWinner' have value 0.
"""

# baseline percentage of defeat - 0
baseline = 1- sum(sample['flWinner'])/sample.shape[0]
baseline

# can also use mean 
# tb_lobby_stats_player['flWinner'].mean()

"""### Logistic regression

"""

#model 1 -> logistic reg  - full set df
from sklearn.linear_model import LogisticRegression

LR = LogisticRegression(random_state=5).fit(X_train, y_train)
ytrain_predict = LR.predict(X_train)
ytest_predict = LR.predict(X_test)

# calculate accuracy
accuracy_train = accuracy_score(ytrain_predict, y_train)
accuraccy_test = accuracy_score(ytest_predict, y_test)

print("accuracy for the training dataset is " + str(accuracy_train))
print("accuracy for the testing dataset is " + str(accuraccy_test))

#model 1 -> logistic reg - selected feature 

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(random_state=5).fit(sub_X_train, y_train)
ytrain_predict = LR.predict(sub_X_train)
ytest_predict = LR.predict(sub_X_test)

# calculate accuracy
accuracy_train = accuracy_score(ytrain_predict, y_train)
accuraccy_test = accuracy_score(ytest_predict, y_test)

print("accuracy for the training dataset is " + str(accuracy_train))
print("accuracy for the testing dataset is " + str(accuraccy_test))

#model 1 -> logistic reg - stdandardized 

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(random_state=5).fit(X1_std_train, y_train)
ytrain_predict = LR.predict(X1_std_train)
ytest_predict = LR.predict(X1_std_test)

# calculate accuracy
accuracy_train = accuracy_score(ytrain_predict, y_train)
accuraccy_test = accuracy_score(ytest_predict, y_test)

print("accuracy for the training dataset is " + str(accuracy_train))
print("accuracy for the testing dataset is " + str(accuraccy_test))

from sklearn.linear_model import LogisticRegressionCV
clf = LogisticRegressionCV(cv=5, random_state=5).fit(X1_std_train, y_train)
prediction = clf.predict(X)
clf.score(X, y)

from sklearn.model_selection import GridSearchCV
params = {'C': [100, 10, 1.0, 0.1],
          'penalty': ['None','l1', 'l2']}

lg = GridSearchCV(LogisticRegression(), params)
lg.fit(X_train, y_train)
lg.best_params_

lg.best_score_

from sklearn.model_selection import RandomizedSearchCV
params = {'C': [100, 10, 1.0, 0.1],
          'penalty': ['None','l1', 'l2']}
lg = RandomizedSearchCV(LogisticRegression(), params)
lg.fit(X_train, y_train)
lg.best_params_

lg.best_score_

from sklearn.model_selection import cross_val_score, KFold

# Declare the inner and outer cross-validation strategies
inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)
outer_cv = KFold(n_splits=3, shuffle=True, random_state=0)

# Inner cross-validation for parameter search
model = GridSearchCV(
    estimator = model_to_tune, param_grid = param_grid, cv = inner_cv, n_jobs=2
)

# Outer cross-validation to compute the testing score
test_score = cross_val_score(model, data, target, cv=outer_cv, n_jobs=2)
print(f"The mean score using nested cross-validation is: "
      f"{test_score.mean():.3f} +/- {test_score.std():.3f}")

# Confustion matrix to measure classification success/show types of errors

cm = confusion_matrix(y_test,ytest_predict)
print(cm[0])
fpr = cm[1][0]/sum(cm[1])
tpr = cm[0][0]/sum(cm[0])
print('False positive rate', fpr)
print('True positive rate', tpr)
sns.heatmap(cm, annot = True, fmt='d', cbar=False, cmap='Blues').set(title = "Confusion Matrix of Logistic Regression")

# ROC Curve to interpret the Logistic Regression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve

def plot_roc_curve(fper, tper):
    plt.plot(fper, tper, color='red', label='ROC')
    plt.plot([0, 1], [0, 1], color='green', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic Curve')
    plt.legend()
    plt.show()
    
model =RandomForestClassifier()
model.fit(X_train, y_train)

prob = model.predict_proba(X_test)
prob = prob[:, 1]
fper, tper, thresholds = roc_curve(y_test, prob)
J = tper - fper
ix = np.argmax(J)
best_thresh = thresholds[ix]
print('Best Threshold = %f' % (best_thresh))
plot_roc_curve(fper, tper)

# Top left of the curve is the ideal.

"""**important**<br />
* standardized df > selected features > full df
* has high accuracy and quit similar w linear regression's best accuracy 
* prob need tunning on it

### random forest
"""

#model 2 ->  randon forest 

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

clf = RandomForestClassifier(max_depth=2, random_state=5)
clf.fit(X, y)

ytrain_predict = clf.predict(X_train)
ytest_predict = clf.predict(X_test)

print("accuracy for the training dataset is " + str(accuracy_score(ytrain_predict, y_train)))
print("accuracy for the testing dataset is " + str(accuracy_score(ytest_predict , y_test)))

importances = list(clf.feature_importances_)
predictor_importances = [(fullcol, round(importance, 2)) for fullcol, importance in zip(fullcol, importances)]
predictor_importances = sorted(predictor_importances, key = lambda x: x[1], reverse = True)
var_names = []
var_val = []
for tup in predictor_importances : 
  var,val = tup
  var_names.append(var)
  var_val.append(val)

plt.style.use('fivethirtyeight')

importances = sorted(importances)
x_values = list(range(len(importances)))
plt.figure(figsize=(13,5))
plt.bar(var_names, var_val, orientation = 'vertical')
plt.xticks(x_values, var_names, rotation='vertical') 
plt.ylabel('Importance')
plt.xlabel('Variable')
plt.title('Variable Importances');

from sklearn import tree
plt.figure(figsize=(15,15)) 
tree.plot_tree(clf.estimators_[22], feature_names=fullcol);

params = {'var_smoothing': np.logspace(0,-9, num=100)}
gs_NB = GridSearchCV(GaussianNB(), params)
gs_NB.fit(X_train, y_train)
gs_NB.best_params_from sklearn.model_selection import GridSearchCV
rfc = RandomForestClassifier()
parameters = {'max_depth':[5,10]}
clf = GridSearchCV(rfc, parameters)
clf.fit(X, y)
GridSearchCV(estimator=rfc, param_grid= parameters )
# sorted(clf.cv_results_.keys())

# Confusion matrix for our random forest
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test, ytest_predict)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label')

"""### decision tree"""

from sklearn.tree import DecisionTreeClassifier
decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
decision_tree = decision_tree.fit(X_train,y_train)

ytrain_predict = decision_tree.predict(X_train)
ytest_predict = decision_tree.predict(X_test)

print("accuracy for the training dataset is " + str(accuracy_score(ytrain_predict, y_train)))
print("accuracy for the testing dataset is " + str(accuracy_score(ytest_predict , y_test)))

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,max_features = 'auto',splitter = 'random')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'random')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', criterion='entropy')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', min_weight_fraction_leaf=0.5)
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', max_features='auto')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', max_leaf_nodes=4)
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', min_impurity_decrease=0.5)
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', class_weight='balanced')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', class_weight='balanced', ccp_alpha=0.5)
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

"""we can tell changing splitter to best helped """

decision_tree = DecisionTreeClassifier(random_state=5, max_depth=5,splitter = 'best', class_weight='balanced')
decision_tree = decision_tree.fit(X_train,y_train)

scores = cross_val_score(decision_tree, X, y, cv=5)
mean(scores)

params = {'criterion':['gini','entropy'],
          'max_depth':[5,10,50,100]}
dt = GridSearchCV(DecisionTreeClassifier(), params)
dt.fit(X_train, y_train)
dt.best_params_

dt.best_score_

from sklearn.model_selection import RandomizedSearchCV
params = {'criterion':['gini','entropy'],
          'max_depth':[5,10,50,100]}
dt = RandomizedSearchCV(DecisionTreeClassifier(), params)
dt.fit(X_train, y_train)
dt.best_params_

dt.best_score_

"""### navie bayes"""

# explain why try NB 

from sklearn.naive_bayes import GaussianNB

GNB = GaussianNB().fit(X_train,y_train)
ytrain_predict = model.predict(X_train)
ytest_predict = model.predict(X_test)

print("accuracy for the training dataset is " + str(accuracy_score(ytrain_predict,y_train)))
print("accuracy for the testing dataset is " + str(accuracy_score(ytest_predict,y_test)))

scores = cross_val_score(GNB, X, y, cv=5)
mean(scores)

params = {'var_smoothing': np.logspace(0,-9, num=100)}
gs_NB = GridSearchCV(GaussianNB(), params)
gs_NB.fit(X_train, y_train)
gs_NB.best_params_

gs_NB.best_score_

from sklearn.model_selection import RandomizedSearchCV
params = {'var_smoothing': np.logspace(0,-9, num=100)}
gs = RandomizedSearchCV(GaussianNB(), params)
gs.fit(X_train, y_train)
gs.best_params_

gs.best_score_

"""### permutation_importance"""

from sklearn.inspection import permutation_importance
result_2 = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2)
sorted_idx = result_2.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result_2.importances[sorted_idx].T, vert=False, labels=X_train.columns[sorted_idx])

plt.figure(figsize=(100, 40))
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Permutation Feature Importance', fontsize=50)
# plt.legend()
plt.show()

"""### fun finding - linear regression"""

# model fun ->  LinearRegression better result w full feature
# normal df w/o outlier 
from sklearn.linear_model import LinearRegression
model = LinearRegression()    

# original dataset w/o outlier 
model.fit(X_train, y_train)            
ytrain_predict = model.predict(X_train)
ytest_predict = model.predict(X_test)

# fit in treshold
# find the best treshold for the testing and use that reshold to calculate training's accuracy 
tre_list = list(range(0,1000))
accuraccy_test = []
# collect accuracy from 100 treshold
for i in tre_list:
  test_pre = np.where(ytest_predict > (i/1000),1,0)
  accuraccy_test.append(accuracy_score(test_pre, y_test))
# find the best treshold  
tre = accuraccy_test.index(max(accuraccy_test))/1000

train_pre= np.where((ytrain_predict > tre),1,0)
accuraccy_train = accuracy_score(train_pre, y_train)

print("The treshold is " + str(tre))
print("accuracy for the training dataset is " + str(accuraccy_train))
print("accuracy for the testing dataset is " + str(max(accuraccy_test)))

# model fun ->  LinearRegression better result w full feature # with standardized df 
from sklearn.linear_model import LinearRegression
model = LinearRegression()    
model.fit(X1_std_train, y_train)            
ytrain_predict = model.predict(X1_std_train)
ytest_predict = model.predict(X1_std_test)


# fit in treshold
# find the best treshold for the testing and use that reshold to calculate training's accuracy 
tre_list = list(range(0,1000))
accuraccy_test = []
# collect accuracy from 100 treshold
for i in tre_list:
  test_pre = np.where(ytest_predict > (i/1000),1,0)
  accuraccy_test.append(accuracy_score(test_pre, y_test))
# find the best treshold  
tre = accuraccy_test.index(max(accuraccy_test))/1000

train_pre= np.where((ytrain_predict > tre),1,0)
accuraccy_train = accuracy_score(train_pre, y_train)

print("The treshold is " + str(tre))
print("accuracy for the training dataset is " + str(accuraccy_train))
print("accuracy for the testing dataset is " + str(max(accuraccy_test)))

# model fun ->  LinearRegression better result w numerical feature 
# normal df w/o outlier 
from sklearn.linear_model import LinearRegression
model = LinearRegression()    

# original dataset w/o outlier 
model.fit(X_train[fullnum_col], y_train)            
ytrain_predict = model.predict(X_train[fullnum_col])
ytest_predict = model.predict(X_test[fullnum_col])

# fit in treshold
# find the best treshold for the testing and use that reshold to calculate training's accuracy 
tre_list = list(range(0,1000))
accuraccy_test = []
# collect accuracy from 100 treshold
for i in tre_list:
  test_pre = np.where(ytest_predict > (i/1000),1,0)
  accuraccy_test.append(accuracy_score(test_pre, y_test))
# find the best treshold  
tre = accuraccy_test.index(max(accuraccy_test))/1000

train_pre= np.where((ytrain_predict > tre),1,0)
accuraccy_train = accuracy_score(train_pre, y_train)

print("The treshold is " + str(tre))
print("accuracy for the training dataset is " + str(accuraccy_train))
print("accuracy for the testing dataset is " + str(max(accuraccy_test)))

"""**important**<br />
* use the testing's best treshold to calculate training's accuracy 
* we shouldnt use linear reg on our model bc its doesnt make sense but it work, so we will talk about it at last as a surprise
* full set of df > full set numerical columns > selected feature >full set standardized df 
* we dont need to tune linear
"""